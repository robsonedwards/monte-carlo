
\documentclass[11pt, oneside]{article}
\usepackage{geometry} 
\geometry{a4paper} 
%\usepackage[parfill]{parskip}  % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}			% Use pdf, png, jpg, or eps§ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{amssymb}
\usepackage{xcolor}

\pagenumbering{gobble}

%SetFonts

%SetFonts

\title{Assignment 3: Report}
\author{Robson Edwards}
\date{November 22, 2018}

\definecolor{codegray}{gray}{0.88}
\newcommand \Rcode[1]{{\texttt{\colorbox{codegray}{#1}}}}
%\newcommand \Rcode[1]{{\texttt{{#1}}}}

\begin{document}

\maketitle

I confirm that the following report and associated code is my own work, except where clearly indicated.

%If you fit a model to some data, consider whether there is a need for goodness-of-fit tests. State the assumptions of the methods you use, and consider how likely they are to be met – see the above advice on the Discussion section for more on this.

\section*{Abstract}

We investigate poll data from the 2016 U.S. Election and attempt to answer the research question ``Do polls become more accurate closer to the election?'' 
To do this, we first devise a measure of poll error. 
We simulate large amounts of data, in various situations, informed by the properties of the real data. 
We then test the correlation between poll date and our error measure, using a parametric test involving Pearson's product moment correlation coefficient, and also a non-parametric test using Spearman's rank correlation coefficient. \cite{R}
We also determine the size and power of our tests. 

\section{Introduction}

We use the \Rcode{polls\_us\_election\_2016} dataset from \Rcode{dslabs}, which contains poll data from FiveThirtyEight which includes ``Poll results from US 2016 presidential elections aggregated from HuffPost Pollster, RealClearPolitics, polling firms and news reports.'' \cite{dslabs} 

We construct an error measure and then make the hypotheses:

\begin{itemize}
\item $H_0$: Poll error mean does not vary with respect to time.

\item $H_1$: Poll error is strictly decreasing with respect to time. 
\end{itemize}

We devise methods for simulating data that might answer these questions and then test these hypotheses and determine the size and power of these tests. 

\section{Methods}

We loaded the aforementioned data and then ``wrangled'' it, removing some unneeded variables and transforming others. We then simulated data based on the properties of the true data and used this to run a Pearson correlation test and a Spearman correlation test and to determine the size and power of those tests in different situations. 
More detail can be found below and in the attached R script. 

\subsection{Data Wrangling}

The aforementioned data includes \Rcode{polls}, 4208 observations (rows) of 15 variables (columns), where each row represents a single poll. The columns include at least five which we ignore for reasons outside the scope of this exercise. The remainder represent variables which we make use of below. 

The data also include the \Rcode{results} of the election, from which we use the two columns and 51 rows representing the popular vote percentages won by Hillary Clinton and Donald Trump in each of the 50 states and in the District of Columbia. Unfortunately, there is not a row for the popular vote in the U.S. as a whole. We will later consider poll error by region. To do this, we need to know the result of the election in each region. 1106 of the 4208 polls in the data have the whole U.S. as their region so we construct a row for the result in the U.S. as follows: ``Clinton's final tally came in at 65,844,610, compared to Donald Trump's 62,979,636... votes for other candidates [were] 7,804,213.'' \cite{time} We thus insert a row with \Rcode{state} U.S., \Rcode{clinton} 48.2\%, \Rcode{trump} 46.1\%.

In \Rcode{polls}, \Rcode{state} is a factor representing where the poll took place. There are 57 levels, including 50 for the 50 states, one for the District of Columbia, and one for the whole country. The remaining five levels refer to specific congressional districts within either Maine or Nebraska. We don't have results for congressional districts so we can't use the 29 polls with these \Rcode{state}s and thus ignore them.

We make two other changes to simplify inference later. We consider \Rcode{enddate} in the unit of days before the election, which is a negative integer rather than a raw date. We also consider the poll duration in days instead of \Rcode{startdate}. 

Now we ought to construct a measure of poll error which will allow us to test our hypotheses. Our first instinct is to use an error measure suggested by Nate Silver \cite{silver}, where he suggests we ``...compare the margin in the poll against the actual result. For instance, if a poll showed the Democrat winning by 2 percentage points in a race that the Republican ended up winning by 3 points, that would be a 5-point error. It would also be a 5-point error if the Democrat won by 7 points. We consider these errors to be equally bad even though the pollster `called' the winner correctly in one case and failed to do so in the other.''

Therefore, we construct a variable \Rcode{error} as the poll margin for that poll minus the result margin for that state. Then, positive \Rcode{error} values correspond to Hillary Clinton losing a state which the pollster expected her to lose by a greater margin than the pollster expected, losing a state when she was expected to win, or winning a state by a lesser margin than expected (or, equivalently, Donald Trump winning a state by a \emph{greater} margin than expected, and so on). Negative values correspond to the above situations except with Clinton's name replaced by Trump's. 

It is not obvious whether a decrease in \Rcode{error} corresponds to an improvement in accuracy. This is certainly not ideal. Therefore, we will construct another variable, \Rcode{error2}, as \Rcode{error \^{} 2}. Now \Rcode{error2} is obviously always non-negative, so a decrease in this measure will always correspond to an improvement in poll accuracy. However, a relationship that is linear on the \Rcode{error} scale is certainly not linear on this scale. 

We also remove 43 ``outlier'' observations which have \Rcode{error} more than three standard deviations away from the mean. After this, \Rcode{error} is approximately normally distributed. 

\subsection{Simulating Data}

The specified goal of this assignment is to generate a large amount of data in different situations, apply our statistical tests to the data, and then determine the \emph{statistical size} and \emph{power} of those tests in those situations. Of course, the size of a test is the probability of incorrectly rejecting $H_0$, given that it is true. Similarly, the power of a test is the probability of correctly rejecting $H_0$ given $H_1$ is true, for a given $H_1$ \cite{spec}. Were we to simulate directly from the data, we would not know whether $H_0$ or $H_1$, as given later, are true, and thus we could not assess the size or power of any test on those hypotheses. 

Instead, we assume either $H_0$ is true or a specific $H_1$ is true and work from there. 

Because \Rcode{error} is approximately normally distributed, we assume it follows a normal distribution with variance constant over time, and mean based on the hypotheses $H_0$ and $H_1$. To determine size, we assume $H_0$ and then sample from a normal distribution with mean and standard deviation equal to that of the population (of polls). We also subset the population (of polls) by FiveThirtyEight poll grade and simulate within those distributions. We then do the same for $H_1$, except we assume a linear relationship between poll error and time. For a small effect, we assume the slope of this line is -0.001. For a medium effect, -0.01. For a large one, -0.05. 

Again, this process is detailed further in the code. 

%Describe what you did, in enough detail that someone else could reproduce it. In academic papers this is sometimes not possible due to space constraints, so full details are relegated to an appendix; sometimes the computer code and input data are archived in an appendix (especially online appendices). For the reports in MT4613 you should be able to explain what you did concisely without the need for appendices. If the methods section is quite long, you may need subheadings to divide it into manageable parts. Don’t be afraid of using formulae here to explain your methods, but make sure you define all the symbols used. It may also be helpful to use a figure, for example to show a map of the study area, or a diagram explaining how various analyses fit together. Such figures will likely not be required in MT4613.

\section{Results}

%say what you found. Results should be given in the same order as the work was laid out in the methods section, with the same (or nearly the same) subheadings as the methods (if you had any). Only report results that are relevant to the problem at hand, and were motivated by the methods section – don’t just dump any old output from a computer program into the results section. Summarize results into tables and figures as much as possible, and don’t repeat information given in tables in the text, just refer to it. For example “All five populations showed a statistically significant decline in numbers after the fire (Table 1).” See below for more advice on tables and figures. Consider how many significant figures/decimal places results should reasonably be reported to, and be consistent – there is often no need to report results to more than 2 or 3 decimal places.

Unfortunately, because we assumed hypotheses, these simulation tests don't actually lead us to any conclusions about the research question. 

We made many other assumptions for the simulation, including normality of the error and independence on many axes. 

\subsection{Size and Power}

The following are the size (for effect 0) and power (for effects -0.001, -0.01, and -0.05), for the Pearson and Spearman tests on all the data, randomly sampled with $n = 1000$. 

\begin{center}
 \begin{tabular}{|c|c c c c|} 
 \hline
 & 0 & -0.001 & -0.01 & -0.05 \\  
 \hline
 Pearson& 0.04 & 0.06 & 0.28 & 1.00 \\ 
 \hline
 Spearman & 0.06 & 0.06 & 0.29 & 1.00 \\
 \hline
 \end{tabular}
\end{center}

As we can see, -0.001 is essentially a negligible effect. -0.01 is a significant one, but not so significant as to make the tests particularly powerful. -0.05 is essentially an absurd effect and it may lead to values of error outside the interval [-100, 100], which cannot exist in nature. 

It is interesting that the power of the parametric Pearson test and the non-parametric Spearman test are pretty close. 

You can find the full list of sizes and powers by running the R script. It is surprisingly fast but it does spam your console. 

%\subsection{Conclusion}

%Conclusion

%What do results mean for the research question? Further research?

%Concisely lay out the main conclusions of the paper. This section is often somewhat redundant, given that these are also given in the abstract (although in less detail). You’re probably better to finish the discussion with a pity [pithy?] concluding paragraph.

\begin{thebibliography}{9}

\bibitem{dslabs}{
 Rafael A. Irizarry (2018). \textit{dslabs: Data Science Labs.} R package
 version 0.5.1. https://CRAN.R-project.org/package=dslabs
}
\bibitem{time}{
Begley, S. (20 Dec. 2016). 
\textit{Hillary Clinton Leads by 2.8 Million in Final Popular Vote Count.} [online] Time. Available at: http://time.com/4608555/hillary-clinton-popular-vote-final/ [Accessed 22 Nov. 2018].
}

\bibitem{silver}{
Silver, N. (30 May 2018). 
\textit{The Polls Are All Right} [online] FiveThirtyEight. Available at: https://fivethirtyeight.com/features/the-polls-are-all-right/ [Accessed 22 Nov. 2018].
}

\bibitem{spec}{
Jacobsen, E. (3 Nov. 2018).
Computing in Statistics: Assignment 3. [Not published].
}

\bibitem{R}{
R Core Team (2018). 
\textit{R: A language and environment for statistical
computing.} R Foundation for Statistical Computing, Vienna, Austria.
URL https://www.R-project.org/.
}
 

\end{thebibliography}

%This section is sometimes called “Literature cited”. Each printed text or web site referred to in the report should be listed here, in a standard format. The format can mimic any standard statistics journal; if in doubt, use the Harvard system of referencing (search on the web for information about this), which is something of a standard in science.

%\section*{Appendix}
%\appendix{} %which one?

\end{document}
